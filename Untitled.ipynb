{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fairRoll(length=sequence_length):\n",
    "    \"\"\" \n",
    "        length: the length of the outputed sequence\n",
    "        Returns a sequnce of farily rolled die\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for _ in range(length):\n",
    "        result.append(random.randint(1,6))\n",
    "    return result\n",
    "\n",
    "def biasRoll(length=sequence_length,bias=[.1,.1,.1,.15,.2,.35]):\n",
    "    \"\"\" \n",
    "      length: the length of the outputed sequence\n",
    "      bias: an array that must sum to 1 reprsenting the weight of each side of dice.\n",
    "      Returns a sequnce of biasily rolled die\n",
    "    \"\"\"\n",
    "    if sum(bias) != 1:\n",
    "        print(\"Bad bias\")\n",
    "        return None\n",
    "    else:\n",
    "        result = []\n",
    "        for _ in range(length):\n",
    "            r = random.random()\n",
    "            threshold = 0\n",
    "            for index,weight in enumerate(bias):\n",
    "                threshold += weight\n",
    "                if r <= threshold:\n",
    "                    result.append(index+1)\n",
    "                    break\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiceDataset(Dataset):\n",
    "    def __init__(self,amount,length):\n",
    "        #Compute data\n",
    "        if amount % 2 != 0:\n",
    "            amount += 1\n",
    "            print(\"Amount has to be even, amount set to:\",amount)\n",
    "        data = []\n",
    "        target = [[1,0],[0,1]]*(amount//2)\n",
    "        for _ in range(amount//2):\n",
    "            data.extend([fairRoll(length=length),biasRoll(length=length)])\n",
    "        #Define paramets\n",
    "        self.len = amount\n",
    "        self.x_data = torch.Tensor(data)\n",
    "        self.y_data = torch.Tensor(target)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "entry_count = int(100000)\n",
    "batch_size = int(100)\n",
    "\n",
    "dataset = DiceDataset(entry_count,sequence_length)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I created feed-forward neural network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiceNet(\n",
      "  (fc1): Linear(in_features=100, out_features=48, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=48, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DiceNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DiceNet,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = DiceNet(sequence_length,48,2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.3363250494003296\n",
      "Loss: 0.36333969235420227\n",
      "Loss: 0.3631409704685211\n",
      "Loss: 0.31232890486717224\n",
      "Loss: 0.3218327462673187\n",
      "Loss: 0.313308447599411\n",
      "Loss: 0.31147661805152893\n",
      "Loss: 0.2912723422050476\n",
      "Loss: 0.3102191686630249\n",
      "Loss: 0.31508904695510864\n",
      "Loss: 0.3020291328430176\n",
      "Loss: 0.29504773020744324\n",
      "Loss: 0.2707083225250244\n",
      "Loss: 0.24810074269771576\n",
      "Loss: 0.2597602605819702\n",
      "Loss: 0.2523333728313446\n",
      "Loss: 0.2612494230270386\n",
      "Loss: 0.2589111030101776\n",
      "Loss: 0.2620314359664917\n",
      "Loss: 0.265176922082901\n",
      "Loss: 0.25768133997917175\n",
      "Loss: 0.2308899462223053\n",
      "Loss: 0.23959596455097198\n",
      "Loss: 0.24056828022003174\n",
      "Loss: 0.23300306499004364\n",
      "Loss: 0.23343515396118164\n",
      "Loss: 0.2249111533164978\n",
      "Loss: 0.20838689804077148\n",
      "Loss: 0.22330981492996216\n",
      "Loss: 0.2209848016500473\n",
      "Loss: 0.24224038422107697\n",
      "Loss: 0.22102871537208557\n",
      "Loss: 0.19781576097011566\n",
      "Loss: 0.22764885425567627\n",
      "Loss: 0.2266377955675125\n",
      "Loss: 0.21778836846351624\n",
      "Loss: 0.20504970848560333\n",
      "Loss: 0.21035268902778625\n",
      "Loss: 0.21097137033939362\n",
      "Loss: 0.21511346101760864\n",
      "Loss: 0.2104860097169876\n",
      "Loss: 0.1921578347682953\n",
      "Loss: 0.17701081931591034\n",
      "Loss: 0.20831748843193054\n",
      "Loss: 0.18781797587871552\n",
      "Loss: 0.19243231415748596\n",
      "Loss: 0.19497385621070862\n",
      "Loss: 0.1627282351255417\n",
      "Loss: 0.20533792674541473\n",
      "Loss: 0.18282125890254974\n",
      "Loss: 0.197356179356575\n",
      "Loss: 0.16988739371299744\n",
      "Loss: 0.21430478990077972\n",
      "Loss: 0.19102703034877777\n",
      "Loss: 0.20972979068756104\n",
      "Loss: 0.16728121042251587\n",
      "Loss: 0.1810600906610489\n",
      "Loss: 0.17998988926410675\n",
      "Loss: 0.18399809300899506\n",
      "Loss: 0.18401995301246643\n",
      "Loss: 0.1633235216140747\n",
      "Loss: 0.18264584243297577\n",
      "Loss: 0.17147542536258698\n",
      "Loss: 0.17951427400112152\n",
      "Loss: 0.14795701205730438\n",
      "Loss: 0.16327893733978271\n",
      "Loss: 0.21692709624767303\n",
      "Loss: 0.17450198531150818\n",
      "Loss: 0.13871362805366516\n",
      "Loss: 0.19131499528884888\n",
      "Loss: 0.148553267121315\n",
      "Loss: 0.1742006093263626\n",
      "Loss: 0.18268251419067383\n",
      "Loss: 0.15849260985851288\n",
      "Loss: 0.18741275370121002\n",
      "Loss: 0.16940580308437347\n",
      "Loss: 0.16635511815547943\n",
      "Loss: 0.17504605650901794\n",
      "Loss: 0.14496462047100067\n",
      "Loss: 0.154988095164299\n",
      "Loss: 0.18736864626407623\n",
      "Loss: 0.18493518233299255\n",
      "Loss: 0.14981557428836823\n",
      "Loss: 0.12988786399364471\n",
      "Loss: 0.1262398660182953\n",
      "Loss: 0.1301203817129135\n",
      "Loss: 0.13614928722381592\n",
      "Loss: 0.16146251559257507\n",
      "Loss: 0.17533443868160248\n",
      "Loss: 0.12011560797691345\n",
      "Loss: 0.13466520607471466\n",
      "Loss: 0.18177197873592377\n",
      "Loss: 0.17188812792301178\n",
      "Loss: 0.14372926950454712\n",
      "Loss: 0.14783260226249695\n",
      "Loss: 0.1245851218700409\n",
      "Loss: 0.14371587336063385\n",
      "Loss: 0.1330450028181076\n",
      "Loss: 0.13134807348251343\n",
      "Loss: 0.19233430922031403\n",
      "Loss: 0.1490209996700287\n",
      "Loss: 0.14006081223487854\n",
      "Loss: 0.1327248066663742\n",
      "Loss: 0.15872664749622345\n",
      "Loss: 0.11694900691509247\n",
      "Loss: 0.13274534046649933\n",
      "Loss: 0.11192826181650162\n",
      "Loss: 0.13341587781906128\n",
      "Loss: 0.14379407465457916\n",
      "Loss: 0.12229739129543304\n",
      "Loss: 0.14055176079273224\n",
      "Loss: 0.13795319199562073\n",
      "Loss: 0.11602389067411423\n",
      "Loss: 0.14058737456798553\n",
      "Loss: 0.1253756582736969\n",
      "Loss: 0.11626631766557693\n",
      "Loss: 0.12391895055770874\n",
      "Loss: 0.12460771203041077\n",
      "Loss: 0.10509434342384338\n",
      "Loss: 0.15028327703475952\n",
      "Loss: 0.1271975189447403\n",
      "Loss: 0.15011970698833466\n",
      "Loss: 0.12118706107139587\n",
      "Loss: 0.10391254723072052\n",
      "Loss: 0.10470765084028244\n",
      "Loss: 0.10730466991662979\n",
      "Loss: 0.09872457385063171\n",
      "Loss: 0.12346475571393967\n",
      "Loss: 0.11656863242387772\n",
      "Loss: 0.11037321388721466\n",
      "Loss: 0.11000385880470276\n",
      "Loss: 0.11842941492795944\n",
      "Loss: 0.12676939368247986\n",
      "Loss: 0.1049434095621109\n",
      "Loss: 0.09772675484418869\n",
      "Loss: 0.10764700174331665\n",
      "Loss: 0.11562605202198029\n",
      "Loss: 0.1810419112443924\n",
      "Loss: 0.09486965835094452\n",
      "Loss: 0.1375652700662613\n",
      "Loss: 0.09079177677631378\n",
      "Loss: 0.10339982807636261\n",
      "Loss: 0.09740370512008667\n",
      "Loss: 0.0979919508099556\n",
      "Loss: 0.11947399377822876\n",
      "Loss: 0.11238588392734528\n",
      "Loss: 0.11371281743049622\n",
      "Loss: 0.11811304092407227\n",
      "Loss: 0.08071748912334442\n",
      "Loss: 0.12460142374038696\n",
      "Loss: 0.10955721884965897\n",
      "Loss: 0.10391862690448761\n",
      "Loss: 0.09887272864580154\n",
      "Loss: 0.11917811632156372\n",
      "Loss: 0.10068240016698837\n",
      "Loss: 0.107139453291893\n",
      "Loss: 0.09790357947349548\n",
      "Loss: 0.09365398436784744\n",
      "Loss: 0.09609918296337128\n",
      "Loss: 0.11610323935747147\n",
      "Loss: 0.1075013130903244\n",
      "Loss: 0.08482615649700165\n",
      "Loss: 0.11077745258808136\n",
      "Loss: 0.08425404876470566\n",
      "Loss: 0.11365765333175659\n",
      "Loss: 0.0818193182349205\n",
      "Loss: 0.10258010029792786\n",
      "Loss: 0.07541565597057343\n",
      "Loss: 0.08034753054380417\n",
      "Loss: 0.08081076294183731\n",
      "Loss: 0.0750817283987999\n",
      "Loss: 0.10598389804363251\n",
      "Loss: 0.08594577759504318\n",
      "Loss: 0.08811011910438538\n",
      "Loss: 0.10999863594770432\n",
      "Loss: 0.18599124252796173\n",
      "Loss: 0.08404751121997833\n",
      "Loss: 0.093390092253685\n",
      "Loss: 0.08731351047754288\n",
      "Loss: 0.08434625715017319\n",
      "Loss: 0.09454484283924103\n",
      "Loss: 0.1559455841779709\n",
      "Loss: 0.09856083989143372\n",
      "Loss: 0.07559552788734436\n",
      "Loss: 0.10398194938898087\n",
      "Loss: 0.13002876937389374\n",
      "Loss: 0.11287052184343338\n",
      "Loss: 0.06168508157134056\n",
      "Loss: 0.06997661292552948\n",
      "Loss: 0.08754736930131912\n",
      "Loss: 0.095210961997509\n",
      "Loss: 0.07951828837394714\n",
      "Loss: 0.06665343046188354\n",
      "Loss: 0.07282676547765732\n",
      "Loss: 0.11333837360143661\n",
      "Loss: 0.06907694786787033\n",
      "Loss: 0.06923123449087143\n",
      "Loss: 0.06731598824262619\n",
      "Loss: 0.056703224778175354\n",
      "Loss: 0.06687691807746887\n",
      "Loss: 0.0693725049495697\n",
      "Loss: 0.07502954453229904\n",
      "Loss: 0.10306453704833984\n",
      "Loss: 0.0624607689678669\n",
      "Loss: 0.11368098855018616\n",
      "Loss: 0.06592792272567749\n",
      "Loss: 0.07525904476642609\n",
      "Loss: 0.08597137778997421\n",
      "Loss: 0.059738244861364365\n",
      "Loss: 0.08931384235620499\n",
      "Loss: 0.06579755991697311\n",
      "Loss: 0.06479837745428085\n",
      "Loss: 0.07606714963912964\n",
      "Loss: 0.05461956188082695\n",
      "Loss: 0.060816165059804916\n",
      "Loss: 0.07355496287345886\n",
      "Loss: 0.05518246814608574\n",
      "Loss: 0.0667356550693512\n",
      "Loss: 0.058451924473047256\n",
      "Loss: 0.07048255205154419\n",
      "Loss: 0.05342073738574982\n",
      "Loss: 0.06369355320930481\n",
      "Loss: 0.07523583620786667\n",
      "Loss: 0.09717300534248352\n",
      "Loss: 0.060541629791259766\n",
      "Loss: 0.08957519382238388\n",
      "Loss: 0.06398867815732956\n",
      "Loss: 0.09598740935325623\n",
      "Loss: 0.08303292095661163\n",
      "Loss: 0.07441346347332001\n",
      "Loss: 0.06169627979397774\n",
      "Loss: 0.07360982894897461\n",
      "Loss: 0.06786829978227615\n",
      "Loss: 0.09997252374887466\n",
      "Loss: 0.05390411242842674\n",
      "Loss: 0.08484326303005219\n",
      "Loss: 0.09928807616233826\n",
      "Loss: 0.06933311372995377\n",
      "Loss: 0.08823112398386002\n",
      "Loss: 0.06268186867237091\n",
      "Loss: 0.062143273651599884\n",
      "Loss: 0.055499523878097534\n",
      "Loss: 0.06014895811676979\n",
      "Loss: 0.060217127203941345\n",
      "Loss: 0.05903725326061249\n",
      "Loss: 0.05647590011358261\n",
      "Loss: 0.05348427593708038\n",
      "Loss: 0.07707219570875168\n",
      "Loss: 0.05920878052711487\n",
      "Loss: 0.060423411428928375\n",
      "Loss: 0.055545467883348465\n",
      "Loss: 0.056914377957582474\n",
      "Loss: 0.06982021033763885\n",
      "Loss: 0.08089282363653183\n",
      "Loss: 0.06250558793544769\n",
      "Loss: 0.08869337290525436\n",
      "Loss: 0.06546566635370255\n",
      "Loss: 0.07851098477840424\n",
      "Loss: 0.05297168344259262\n",
      "Loss: 0.05479434132575989\n",
      "Loss: 0.0542885884642601\n",
      "Loss: 0.05419031158089638\n",
      "Loss: 0.08830925822257996\n",
      "Loss: 0.06131701171398163\n",
      "Loss: 0.04774149879813194\n",
      "Loss: 0.03223282843828201\n",
      "Loss: 0.06084293872117996\n",
      "Loss: 0.09181306511163712\n",
      "Loss: 0.05495667830109596\n",
      "Loss: 0.06874097883701324\n",
      "Loss: 0.04323314502835274\n",
      "Loss: 0.05440850183367729\n",
      "Loss: 0.05283062532544136\n",
      "Loss: 0.05320030823349953\n",
      "Loss: 0.04244247078895569\n",
      "Loss: 0.07018914818763733\n",
      "Loss: 0.05180912837386131\n",
      "Loss: 0.054240647703409195\n",
      "Loss: 0.05561620742082596\n",
      "Loss: 0.053731150925159454\n",
      "Loss: 0.05303549766540527\n",
      "Loss: 0.08946727961301804\n",
      "Loss: 0.0545501746237278\n",
      "Loss: 0.0508524551987648\n",
      "Loss: 0.061006493866443634\n",
      "Loss: 0.05230738967657089\n",
      "Loss: 0.055107031017541885\n",
      "Loss: 0.053653594106435776\n",
      "Loss: 0.055670324712991714\n",
      "Loss: 0.07122118026018143\n",
      "Loss: 0.04430074617266655\n",
      "Loss: 0.04549911990761757\n",
      "Loss: 0.04845331981778145\n",
      "Loss: 0.06026056408882141\n",
      "Loss: 0.053423501551151276\n",
      "Loss: 0.05572058632969856\n",
      "Loss: 0.06654196977615356\n",
      "Loss: 0.08292066305875778\n",
      "Loss: 0.06503313779830933\n",
      "Loss: 0.057360682636499405\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#define critierion and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#training cycle\n",
    "for epoch in range(3):\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        #get the input & labels\n",
    "        inputs,labels = data\n",
    "        #print(\"Inputs:\",inputs)\n",
    "        #print(\"Labels:\",labels)\n",
    "        \n",
    "        #zero gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward, backward, optimize\n",
    "        output = model(inputs)\n",
    "        #print(\"Output:\",output)\n",
    "        loss = criterion(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print statistics\n",
    "        if i%10 == 0:\n",
    "            print(\"Loss:\",loss.item())\n",
    "        \n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess(roll):\n",
    "    \"\"\"\n",
    "    Input: roll (Array), string of numbers produced by a die\n",
    "    Output: whether it is fair or biased\n",
    "    \"\"\"\n",
    "    x = torch.Tensor(roll)\n",
    "    output = model(x)\n",
    "    result = output.tolist()\n",
    "    if result.index(max(result)) == 0:\n",
    "        return \"Fair\"\n",
    "    else:\n",
    "        return \"Bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.83 % accuracy!\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "trials = 10000\n",
    "for i in range(trials):\n",
    "    roll = []\n",
    "    target = ''\n",
    "    if i % 2 == 0:\n",
    "        roll = fairRoll()\n",
    "        target = 'Fair'\n",
    "    else:\n",
    "        roll = biasRoll()\n",
    "        target = 'Bias'\n",
    "    if guess(roll) == target:\n",
    "        correct += 1\n",
    "print(correct/trials*100,\"% accuracy!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
